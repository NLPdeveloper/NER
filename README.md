自2018年以来，以BERT、GPT等为代表的大规模预训练模型已经成为人工智能领域中的重要突破。这些模型具有广泛的适用性和优异的迁移能力，已经成为驱动大规模参数化发展的主要力量。许多知名公司，例如微软、谷歌、Facebook和NVIDIA等，都在投入大量资源研究预训练算法。在国内，百度、华为和阿里等公司也加入了这个领域。虽然中文自然语言处理方向上的预训练模型不断涌现，但这些算法仅依赖于纯文本学习，缺乏知识指导学习，因此模型能力存在一定的限制。


我们所采用的模型输入是SSL+文本，输出是SEL，为了将不同的信息抽取任务结构转化成统一的表示，也就是将不同抽取任务的输出结构转化为统一的格式输出SEL，然后通过解析得到具体的信息，我们将输出结构的生成分解为两个原子操作。<br>
 a) Spotting，标记来自原文的目标信息的片段，例如实体或者事件的触发词。 <br>
 b) Associating，显示存在特定关系的不同信息片段之间的联系，例如不同实体之间的关系，或者事件跟角色的联系。<br>
 
具体的，我们通过三种类型的语义单元来实现这种表述SEL，通过这种方式，可以实现用统一的原子结构生成操作来表示不同的信息抽取任务结构。这三种语义单元分别为： <br>
a) SpotName，显示存在原文中属于spot name这种类型的特定信息的片段。可以理解为实体类型或者事件类型，<br>
b) AssoName，显示存在跟SpotName存在AssoName这种联系的信息片段的。可以理解为关系，<br>
c) InfoSpan，跟上面这种语义单元关联的原文片段。其实就是以上两种语义单元的具体取值。<br>

